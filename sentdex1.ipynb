{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentdex1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9hnqHu7GZSY5VWiu0dB9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hm06063/hihi/blob/master/sentdex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNGRvsoayrs4",
        "colab_type": "text"
      },
      "source": [
        "필기용,, 근데 에러떠서 고민하ㅏ다 포기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9c8NLzMSLpq",
        "colab_type": "code",
        "outputId": "8b607851-27a7-4d51-a308-e3b746539018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "env.reset() #꼭 해줘야함\n",
        "\n",
        "#print(env.observation_space.high)\n",
        "#print(env.observation_space.low)\n",
        "#print(env.action_space.n) # how many actions are possible\n",
        "\n",
        "LEARNING_RATE = 0.1 # 학습률. (상수) 0~1사이값으로 지정하면 됨. 일반적으로는 학습률을 저하시켜\n",
        "DISCOUNT = 0.95 #weight(가중치) -> measure of how important do we find future actions over current actions basically or future reward vs current reward\n",
        "#cuz the way the agent work always going to go off the max Q value 항상 max 큐 값으로 가버림.\n",
        "#your max Q vqlue도 always looking ahead to future max Q values -> that's gonna back propagate all the way down for a long~ chain\n",
        "#어쨌든 저 DISCOUNT 값 = how much we value future reward over current reward. 0~1사이 값.\n",
        "EPISODE = 2000\n",
        "\n",
        "SHOW_EVERY = 500 #let us know that you're still alive\n",
        "\n",
        "#measure of how much random this you want to do or how much exploration you want to do\n",
        "epsilon = 0.5 #chance. 0~1. 높을 수록 perform a random action&exploratory it\n",
        "START_EPSILON_DECAYING = 1\n",
        "END_EPSILON_DECAYING = EPISODE//2 #// - int로 나누기\n",
        "\n",
        "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)# amount that we want to decay by 각 에피소드 \n",
        "\n",
        "#discrete하게 만들어줌. 최고점에서 최저점까지 20바이 20으로 나눠주기\n",
        "#DISCRETE_OS_SIZE = [20,20] - 감당 가능한 크기로 바꿔줌. 항상 같은 크기일 필요는 ㅇ없음.\n",
        "DISCRETE_OS_SIZE = [20]*len(env.observation_space.high) #functional하고 모든 환경에서 가능하게 하고싶기 때문에.\n",
        "\n",
        "#how big their chuncks?\n",
        "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
        "\n",
        "#print(discrete_os_win_size) \n",
        "\n",
        "#finally create the Q table!!\n",
        "# what are the Q values for these actions' combination\n",
        "# what's highest values ? \n",
        "# over time, agents gonna explore and pick random value. using the Q func \n",
        "#-> slowly back propagate to change Q value to lovely reward\n",
        "# 우리는 그전에 초기화를 해줘야대\n",
        "\n",
        "#DISCRETE~SIZE는 20바이20이엇어 -> contains every combinations of position, velocity.\n",
        "# every single action을 보기 위해 +[env.action_space.n]\n",
        "q_table = np.random.uniform(low=-2,high=0,size = (DISCRETE_OS_SIZE)+[env.action_space.n]) #these are two variables . \n",
        "#우리가 받을 rewards 때문에 . reward는 항상 음수야. 니가 flag에 도착하기 전까지!\n",
        "#도착하면 reward 0될거야. 그래서 테이블을 음수로 만드는 거지. ??\n",
        "#print(q_table.shape)\n",
        "\n",
        "ep_rewards = [] # 각 에피소드 reward저장\n",
        "aggr_ep_rewards = {'ep':[],'avg':[], 'min':[],'max':[]}\n",
        " #'ep':gonna track the episode number basically XY역할\n",
        " #'avg':will trailing average. average for any given window so every 500 episodes for ex. average over time so as our model imporves average should go up\n",
        " #'min : track for every what was worst model we had\n",
        " #'max' : best one\n",
        " # avg might actually be going up but the min or worst performing model is still in the dump\n",
        " # and so you might have the cases where you actually prefer that the 최악의 모델이 still somewhat decent then to have highest avg or something like that\n",
        "\n",
        "\n",
        "#q table 이산적으로  만들어주는 함수\n",
        "def get_discrete_state(state):\n",
        "  discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
        "  return tuple(discrete_state.astype(np.int))\n",
        "\n",
        "\n",
        "for episode in range(EPISODE):#iterate\n",
        "  episode_reward = 0\n",
        "\n",
        "  if episode % SHOW_EVERY == 0:\n",
        "    print(episode)\n",
        "    render = True\n",
        "  else:\n",
        "    render = False\n",
        "\n",
        "  discrete_state = get_discrete_state(env.reset()) #env.step() return 4 values, env.reset() returns just initial state\n",
        "  print(discrete_state)\n",
        "  print(np.argmax(q_table[discrete_state])) #Q values. starting values (random and meanlingless) \n",
        "  #-> 최대 값 갖고 싶으니까 argmax함수 써줘! 아마 0이 될테니까 action 0랑 가자~!\n",
        "\n",
        "  done = False \n",
        "\n",
        "  while not done: #환경통해 step하기 위해선 action 필요\n",
        "\n",
        "    if np.random()>epsilon:\n",
        "     action = np.argmax(q_table[discrete_state]) #agent때문에 알필요는 없지만 mountain은 액션이 3개 이써~! 1개는 암것도 안하고 1개는 push car right\n",
        "    else:\n",
        "      action = np.random.randint(0,env.actin_space.n)\n",
        "    new_state,reward,done,_ = env.step(action) #step할 때마다 환경으로부터 new state를 받음.\n",
        "    #your model 에서 value가 뭔지 알 필요없다 하지만 agent에는 상관없지만 그냥 니 위해서 알려주면 포지션이랑 ~야\n",
        "    episode_reward+=reward\n",
        "\n",
        "\n",
        "    new_discrete_state = get_discrete_state(new_state)\n",
        "  # print(reward,new_state) #1\n",
        "    if render:\n",
        "      env.render #cuz rendering the 환경은 너무 느려. 아니면 환경 run 되게 빠를텐데.\n",
        " \n",
        "    if not done:\n",
        "      max_future_q = np.max(q_table[new_discrete_state]) #왜 argmax말고 max썼냐? gonna use max future Q in our new Q 식이므로 Arg max가 뭔지 알기보다는 그 max값을 알고싶은 거얌\n",
        "      #Q value gets back propagated down that down the table so  여기가 중요한 곳이얌~!~!\n",
        "      current_q = q_table[discrete_state+(action, )]#grab the current Q value\n",
        "      #discrete_state만 하면 값 3개나오고 action 하면 the Q value 나옴\n",
        "\n",
        "      #모든 Q값 계산하는 식. DISCOUNT * max_future_q가 back propagate하는 방향에 근거함.\n",
        "      new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT  *max_future_q) \n",
        "      #새 q값으로 q 테이블 업뎃\n",
        "      q_table[discrete_state + (action, )] = new_q \n",
        "    elif new_state[0] >= env.goal_position:#new_state[0] - have position&velocity\n",
        "      q_table[discrete_state + (action, )] = 0 # -> reward for completing things(no punishment)\n",
        "\n",
        "    discrete_state = new_discrete_state\n",
        "  \n",
        "  if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
        "    epsilon -= epsilon_decay_value\n",
        "  ep_rewards.append(episode_reward)\n",
        "\n",
        "#우리 이제 dictionary 쓸 거니까 calculate the avg reard\n",
        " \n",
        "  if not episode%10:\n",
        "    np.save(f\"qtables/{episode}-qtable.py\",q_table)\n",
        "  if not episode % SHOW_EVERY:\n",
        "    average_reward = sum(ep_rewards[-SHOW_EVERY:])/len(ep_rewards[-SHOW_EVERY:]) #-SHOW EVERY : just means like the last 500\n",
        "    aggr_ep_rewards['ep'].append(episode)\n",
        "    aggr_ep_rewards['avg'].append(average_reward)\n",
        "    aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
        "    aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
        "\n",
        "    print(f\"Epsiode: {episode} avg:{average_reward} min:{min(ep_rewards[-SHOW_EVERY:])} max:{max(ep_rewards[-SHOW_EVERY:])}\")\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "plt.plot(aggr_ep_rewards['ep'],aggr_ep_rewards['avg'],label=\"avg\")\n",
        "plt.plot(aggr_ep_rewards['ep'],aggr_ep_rewards['min'],label=\"min\")\n",
        "plt.plot(aggr_ep_rewards['ep'],aggr_ep_rewards['max'],label=\"max\")\n",
        "plt.legent(loc=4) # location  4: lower right\n",
        "plt.show()\n",
        "\n",
        "#Q테이블 - 모든 상태(포지션,필로소티)들의 combination를 만들어서 보기만 해! 그중에서 largest value찾아\n",
        "#values are random and useless -> explore a lot = do random stuff, and slowly update q values\n",
        "#1. build the Q table\n",
        "#2. these values : huge table! many episodes. 너무 오래걸릴거기 때문에 연속적 값에서 이산적 값으로 바꿀거야\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "(7, 10)\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7a7057dd96a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#환경통해 step하기 위해선 action 필요\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m      \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiscrete_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#agent때문에 알필요는 없지만 mountain은 액션이 3개 이써~! 1개는 암것도 안하고 1개는 push car right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    }
  ]
}